{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8464584,"sourceType":"datasetVersion","datasetId":5046305},{"sourceId":8672211,"sourceType":"datasetVersion","datasetId":5197536}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-08-14T11:59:17.179080Z","iopub.execute_input":"2023-08-14T11:59:17.179497Z","iopub.status.idle":"2023-08-14T11:59:28.192761Z","shell.execute_reply.started":"2023-08-14T11:59:17.179457Z","shell.execute_reply":"2023-08-14T11:59:28.191332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nimport segmentation_models_pytorch as smp\nimport segmentation_models_pytorch.utils as smp_utils\nimport numpy as np\nimport rasterio\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.sparse import load_npz\nimport cv2\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport scipy.sparse as sp\nfrom timm.layers import trunc_normal_\nDEVICE = 'cuda'\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom segmentation_models_pytorch.utils.meter import AverageValueMeter\nfrom tqdm import tqdm as tqdm\nimport sys\nimport pickle\nimport networkx as nx","metadata":{"execution":{"iopub.status.busy":"2023-08-14T11:59:28.196789Z","iopub.execute_input":"2023-08-14T11:59:28.197106Z","iopub.status.idle":"2023-08-14T11:59:28.204745Z","shell.execute_reply.started":"2023-08-14T11:59:28.197075Z","shell.execute_reply":"2023-08-14T11:59:28.203767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list = np.load(\"/kaggle/input/data-split-water/train.npy\")\ntest_list = np.load(\"/kaggle/input/data-split-water/valid.npy\")\nval_list = np.load(\"/kaggle/input/data-split-water/test.npy\")\nclass CustomDataset(Dataset):\n    def __init__(self, graph_root_dir,label_root_dir,num_list):\n        self.features = []\n        self.mapping = []\n        self.label = []\n        self.images = []\n        self.adj = []\n        # 遍历文件夹，读取 pkl 文件和 npz 文件\n        for i in range(len(num_list)):\n            self.features.append(graph_root_dir+\"/water_\"+str(num_list[i])+\"_superpixel.npy\")\n            self.adj.append(graph_root_dir+\"/water_\"+str(num_list[i])+\"_adj.npz\")\n            self.mapping.append(graph_root_dir+\"/\"+\"water_\"+str(num_list[i])+\".npz\")\n            self.label.append(label_root_dir+\"/\"+\"water_\"+str(num_list[i])+\".png\")\n            self.images.append(label_root_dir+\"/\"+\"sar_image_\"+str(num_list[i])+\".tif\")\n        self.transformer = transforms.Compose([\n            transforms.ToTensor(),\n        ]) \n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n\n        with rasterio.open(self.images[idx]) as src:\n            data = src.read()\n        image = data[:3] \n        image = image.transpose(1, 2, 0)\n        image = self.transformer(image)\n        # 超像素到像素级映射——\n        feature = np.load(self.features[idx]).astype(np.float32)\n        feature = self.normalize_columns(feature)\n        adj_matrix_loaded = load_npz(self.adj[idx])\n\n        row = torch.LongTensor(adj_matrix_loaded.row)\n        col = torch.LongTensor(adj_matrix_loaded.col)\n        val = torch.FloatTensor(adj_matrix_loaded.data)\n        size = torch.Size(adj_matrix_loaded.shape)\n        adj = torch.sparse_coo_tensor(indices=torch.stack([row, col]), values=val, size=size)\n\n        mapping_matrix = load_npz(self.mapping[idx])\n        \n        row = torch.LongTensor(mapping_matrix.row)\n        col = torch.LongTensor(mapping_matrix.col)\n        val = torch.FloatTensor(mapping_matrix.data)\n        size = torch.Size(mapping_matrix.shape)\n        mapping = torch.sparse_coo_tensor(indices=torch.stack([row, col]), values=val, size=size)\n    \n    \n        mask = cv2.imread(self.label[idx],0).astype(np.float32)\n        mask = mask.reshape(1, mask.shape[0], mask.shape[1])\n        return ((image,feature,adj,mapping),mask)\n    \n    \n    def normalize_columns(self,matrix):\n        \"\"\"\n        将矩阵的每一列归一化到 [0, 1] 的范围内\n        \"\"\"\n        col_max = np.max(matrix, axis=0)   # 获取每一列的最大值\n        col_min = np.min(matrix, axis=0)   # 获取每一列的最小值\n        denominator = col_max - col_min   # 计算每一列的归一化因子\n        denominator[denominator == 0] = 1  # 避免除数为 0 的情况\n        return (matrix - col_min) / denominator   # 归一化操作并返回结果\n\nbatch_size = 8\ntrain_dataset = CustomDataset(\"/kaggle/input/train-image-512/train_image_512\",\"/kaggle/input/image-graph-data\",train_list)  # 假设所有数据文件都在 \"data\" 文件夹下\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = CustomDataset(\"/kaggle/input/train-image-512/train_image_512\",\"/kaggle/input/image-graph-data\",test_list)  # 假设所有数据文件都在 \"data\" 文件夹下\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nval_dataset = CustomDataset(\"/kaggle/input/train-image-512/train_image_512\",\"/kaggle/input/image-graph-data\",val_list)  # 假设所有数据文件都在 \"data\" 文件夹下\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n# for image,label in test_dataloader:\n#     print(image[0].dtype,image[1].dtype,image[2].dtype,image[3].dtype)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:00:38.138975Z","iopub.execute_input":"2023-08-14T12:00:38.139371Z","iopub.status.idle":"2023-08-14T12:00:38.183234Z","shell.execute_reply.started":"2023-08-14T12:00:38.139340Z","shell.execute_reply":"2023-08-14T12:00:38.182367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#卷积块 ——3*3卷积 +batchnorm + relu\nclass Conv_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Conv_block, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size = 3, padding=1,bias = False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n        #初始化模型参数\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n    \n#卷积块 ——1*1卷积 +batchnorm + relu\nclass Conv_block_1(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Conv_block_1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size = 1,bias = False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n        #初始化模型参数\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n    \n#1*1卷积 \nclass Conv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size = 1,bias = False)\n        )\n        #初始化模型参数\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)   \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n    \n#上采样模块（包括上采样+拼接+卷积块）\nclass Up(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Up, self).__init__()\n        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n        self.conv = Conv_block(out_ch*2, out_ch)\n \n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\n#多头注意力机制实现；数为qkv及mask；输出强化+残差连接后特征；\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n    def __init__(self, n_head, d_model, qkv_channels, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.qkv_channels = qkv_channels\n        self.scale = qkv_channels ** 0.5\n        \n        self.w_qkv = nn.Linear(d_model, 3*n_head * qkv_channels, bias=False)\n        self.fc = nn.Linear(n_head * qkv_channels, d_model, bias=False)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self,x, mask=None):\n\n        N,L,C = x.shape\n        \n        residual = x\n        \n        x = self.layer_norm(x)\n        \n        qkv = self.w_qkv(x).reshape(N,L, 3, self.n_head, self.qkv_channels).permute(2, 0, 3, 1, 4)\n        \n        q, k, v = qkv.unbind(0)\n        q = q /self.scale\n        attn = q @ k.transpose(-2, -1)\n        \n        if mask is not None:\n            mask = mask.unsqueeze(1).to_dense()\n            attn = attn.masked_fill(mask == 0, -1e9)\n            \n        attn = attn.softmax(dim=-1)\n        attn = self.dropout1(attn)\n        x = attn @ v\n        \n        x = x.transpose(1, 2).reshape(N,L,-1)\n        x = self.dropout2(self.fc(x))\n        residual = x + residual\n\n        return residual\n#多层感知机\nclass PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n#         self.ls1 = LayerScale(d_in, init_values=0.1) \n\n    def forward(self, x, adj = None):\n\n        residual = x\n        \n        x = self.layer_norm(x)\n        x = self.w_1(x)\n        x = F.relu(adj@x)\n        x = self.w_2(x)\n        x = adj@x\n        x = self.dropout(x)\n#         x = self.ls1(self.dropout(x))\n        x += residual\n        \n        return x\n    \nclass SuperPixelBlock(nn.Module):\n    ''' Compose with two layers '''\n    #d_model 代表模型输入特征维度；d_inner代表多层感知机之间映射特征维度；\n    #n_head代表头个数；d_k, d_v分别代表key和value的特征维度（每个头下的）\n    def __init__(self, d_model, d_inner, n_head, qkv_channels, dropout=0.1):\n        super(SuperPixelBlock, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, qkv_channels, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, slf_attn_mask=None):\n        enc_output = self.slf_attn(\n            enc_input)\n        enc_output = self.pos_ffn(enc_output,slf_attn_mask)\n        return enc_output\n    \nclass SPTUNet(nn.Module):\n    def __init__(self, projection_dim, transformer_layers, transformer_heads,qkv_dim, hidden_dim,dropout):\n        super(SPTUNet, self).__init__()\n        self.projection_dim = projection_dim\n        self.transformer_layers = transformer_layers\n        self.transformer_heads = transformer_heads\n            \n        # 像素级模块\n        self.encoder = timm.create_model(\"efficientnet_b4\",pretrained = True,in_chans=3,features_only = True,drop_rate = 0.2,drop_path_rate=0.2)\n        self.up6 = Up(448, 160)\n        self.up7 = Up(160, 56)\n        self.up8 = Up(56, 32)\n        self.up9 = Up(32, 24)\n        self.up10 = nn.ConvTranspose2d(24,24,2,stride = 2)\n        \n        #超像素级模块\n        # 多层 Transformer 编解码器\n        self.transformers = nn.ModuleList()\n        for _ in range(transformer_layers):\n            self.transformers.append(SuperPixelBlock(projection_dim, hidden_dim, transformer_heads, qkv_dim, dropout=dropout))\n            \n        self.layer_norm = nn.LayerNorm(projection_dim, eps=1e-6)\n        self.graph_outputs = nn.Linear(projection_dim, 24, bias=True)\n        self.pos_embed = nn.Parameter(torch.randn(1, 512, projection_dim) * .02)\n        trunc_normal_(self.pos_embed, std=.02)\n        self.graph_inputs = nn.Linear(5, projection_dim)\n        self.batch_norm = nn.BatchNorm2d(24)\n        \n        #最终输出模块\n        self.output_conv = Conv_block(24,24)\n        self.conv_1 = Conv(24,1)\n        \n    def forward(self,images,graph_inputs, adj_inputs, mapping_matrix):\n        \n        #像素级提取水体：：：\n        c1,c2,c3,c4,c5 = self.encoder(images)\n        up_6 = self.up6(c5, c4)\n        up_7 = self.up7(up_6, c3)\n        up_8 = self.up8(up_7, c2)\n        up_9 = self.up9(up_8, c1)\n        pixels = self.up10(up_9)\n\n        #超像素级提取水体：：：\n        adj_inputs = adj_inputs.to_dense()\n        nbatches = graph_inputs.size(0)\n        #特征映射 +LN\n        superPixels = self.graph_inputs(graph_inputs) \n        superPixels = self.layer_norm(superPixels)\n        #添加位置编码\n        superPixels = superPixels + self.pos_embed\n        # 分类头\n        for transformer in self.transformers:\n            superPixels = transformer(superPixels,adj_inputs)\n        #特征映射 +尺寸映射\n        \n        superPixels = self.graph_outputs(superPixels)\n        superPixels = torch.bmm(mapping_matrix,superPixels)\n        superPixels = torch.reshape(superPixels, [nbatches,24,512, 512])\n        superPixels = self.batch_norm(superPixels)\n        \n        #输出模块：：：\n        outputs = torch.add(pixels,superPixels)\n        outputs = self.output_conv(outputs)\n        outputs = self.conv_1(outputs)\n        outputs = torch.sigmoid(outputs)\n        \n        return outputs\n    \nmodel = SPTUNet(projection_dim=128,  transformer_layers=8, transformer_heads=4, qkv_dim = 32, hidden_dim=512,dropout = 0.2)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T11:59:28.248270Z","iopub.execute_input":"2023-08-14T11:59:28.248718Z","iopub.status.idle":"2023-08-14T11:59:28.949528Z","shell.execute_reply.started":"2023-08-14T11:59:28.248686Z","shell.execute_reply":"2023-08-14T11:59:28.948542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchinfo import summary\n\n# summary(model, [(2,3,512,512),(2,512,5),(2,512,512),(2,262144,512)])","metadata":{"execution":{"iopub.status.busy":"2023-08-14T11:59:28.952531Z","iopub.execute_input":"2023-08-14T11:59:28.952888Z","iopub.status.idle":"2023-08-14T11:59:28.957058Z","shell.execute_reply.started":"2023-08-14T11:59:28.952854Z","shell.execute_reply":"2023-08-14T11:59:28.956074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定义早退函数\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=10, verbose=False, delta=0):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 10\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model):\n        score = val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            print(f'Validation iou increased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model, './best_model.pth')\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-08-14T11:59:28.958691Z","iopub.execute_input":"2023-08-14T11:59:28.959331Z","iopub.status.idle":"2023-08-14T11:59:28.970764Z","shell.execute_reply.started":"2023-08-14T11:59:28.959296Z","shell.execute_reply":"2023-08-14T11:59:28.969607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BCE_BinaryDiceLoss(torch.nn.Module):\n    def __init__(self, ignore_index=None,w = 0.6, reduction='mean', **kwargs):\n        super(BCE_BinaryDiceLoss, self).__init__()\n        self.smooth = 1  # suggest set a large number when target area is large,like '10|100'\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.w = w #代表BCE和diceLoss之间的权重比例\n        self._name = \"BCE_BinaryDiceLoss\" # 新增name属性并设置为该类的名称\n    @property\n    def __name__(self):\n        if self._name is None:\n            name = self.__class__.__name__\n            s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n            return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n        else:\n            return self._name\n    def forward(self, output, target, use_sigmoid=False):\n        assert output.shape[0] == target.shape[0], \"output & target batch size don't match\"\n        \n        if use_sigmoid:\n            output = torch.sigmoid(output)\n\n        if self.ignore_index is not None:\n            validmask = (target != self.ignore_index).float()\n            output = output.mul(validmask)  # can not use inplace for bp\n            target = target.float().mul(validmask)\n\n        dim0= output.shape[0]\n\n        output = output.contiguous().view(dim0,-1)\n        target = target.contiguous().view(dim0,-1).float()\n\n        num = 2 * torch.sum(torch.mul(output, target), dim=1) + self.smooth\n        den = torch.sum(output.abs() + target.abs(), dim=1) + self.smooth\n\n        loss = 1 - (num / den)\n\n        if self.reduction == 'mean':\n            return self.w * F.binary_cross_entropy(output, target)+(1-self.w)*loss.mean()\n        elif self.reduction == 'sum':\n            return self.w * F.binary_cross_entropy(output, target)+(1-self.w)*loss.sum()\n        elif self.reduction == 'none':\n            return self.w * F.binary_cross_entropy(output, target)+(1-self.w)*loss\n        else:\n            raise Exception('Unexpected reduction {}'.format(self.reduction))\n            \nloss = BCE_BinaryDiceLoss()\n\n\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n    smp.utils.metrics.Precision(threshold=0.5),\n    smp.utils.metrics.Recall(threshold=0.5)\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0003),\n])\nscheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\nearly_stopping = EarlyStopping(patience=30, verbose=True)\n\nclass ValidEpoch_avert(smp.utils.train.Epoch):\n    def __init__(self, model, loss, metrics, device=\"cpu\", verbose=True):\n        super().__init__(\n            model=model,\n            loss=loss,\n            metrics=metrics,\n            stage_name=\"valid\",\n            device=device,\n            verbose=verbose,\n        )\n    def batch_update(self, x, y):\n        with torch.no_grad():\n            prediction = self.model.forward(x[0],x[1],x[2],x[3])\n            loss = self.loss(prediction, y)\n        return loss, prediction\n    def on_epoch_start(self):\n        self.model.eval()\n    def run(self, dataloader):\n        self.on_epoch_start()\n        logs = {}\n        loss_meter = AverageValueMeter()\n        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n\n        with tqdm(\n            dataloader,\n            desc=self.stage_name,\n            file=sys.stdout,\n            disable=not (self.verbose),\n        ) as iterator:\n            for x, y in iterator:\n                for i in range(len(x)):\n                    x[i] = x[i].to(self.device)\n                y = y.to(self.device)\n                loss, y_pred = self.batch_update(x, y)\n\n                # update loss logs\n                loss_value = loss.cpu().detach().numpy()\n                loss_meter.add(loss_value)\n                loss_logs = {self.loss.__name__: loss_meter.mean}\n                logs.update(loss_logs)\n\n                # update metrics logs\n                for metric_fn in self.metrics:\n                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n                    metrics_meters[metric_fn.__name__].add(metric_value)\n                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n                logs.update(metrics_logs)\n\n                if self.verbose:\n                    s = self._format_logs(logs)\n                    iterator.set_postfix_str(s)\n\n        return logs\n    \nclass TrainEpoch_avert(smp.utils.train.Epoch):\n    def __init__(self, model, loss, metrics, optimizer,scheduler, device=\"cpu\", verbose=True):\n        super().__init__(\n            model=model,\n            loss=loss,\n            metrics=metrics,\n            stage_name=\"train\",\n            device=device,\n            verbose=verbose,\n        )\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def on_epoch_start(self):\n        self.model.train()\n\n    def batch_update(self, x, y):\n        self.optimizer.zero_grad()\n        prediction = self.model.forward(x[0],x[1],x[2],x[3])\n        loss = self.loss(prediction, y)\n        loss.backward()\n        self.optimizer.step()\n        self.scheduler.step()\n        return loss, prediction\n    \n    def run(self, dataloader):\n        self.on_epoch_start()\n        logs = {}\n        loss_meter = AverageValueMeter()\n        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n\n        with tqdm(\n            dataloader,\n            desc=self.stage_name,\n            file=sys.stdout,\n            disable=not (self.verbose),\n        ) as iterator:\n            for x, y in iterator:\n                for i in range(len(x)):\n                    x[i] = x[i].to(self.device)\n                y = y.to(self.device)\n                loss, y_pred = self.batch_update(x, y)\n                # update loss logs\n                loss_value = loss.cpu().detach().numpy()\n                loss_meter.add(loss_value)\n                loss_logs = {self.loss.__name__: loss_meter.mean}\n                logs.update(loss_logs)\n\n                # update metrics logs\n                for metric_fn in self.metrics:\n                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n                    metrics_meters[metric_fn.__name__].add(metric_value)\n                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n                logs.update(metrics_logs)\n\n                if self.verbose:\n                    s = self._format_logs(logs)\n                    iterator.set_postfix_str(s)\n\n        return logs\n    \ntrain_epoch = TrainEpoch_avert(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    scheduler = scheduler,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = ValidEpoch_avert(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\n\nfor i in range(0, 300):   \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_dataloader)\n    valid_logs = valid_epoch.run(val_dataloader)\n    val_iou = valid_logs['iou_score']\n    early_stopping(val_iou, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"execution":{"iopub.status.busy":"2023-08-14T12:00:42.566357Z","iopub.execute_input":"2023-08-14T12:00:42.566885Z","iopub.status.idle":"2023-08-14T12:03:30.978442Z","shell.execute_reply.started":"2023-08-14T12:00:42.566849Z","shell.execute_reply":"2023-08-14T12:03:30.976667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('/kaggle/input/spt-unet-512/spt_Unet_512_80.pth')\ntest_epoch = ValidEpoch_avert(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\ntest_logs = test_epoch.run(test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T11:59:29.458095Z","iopub.status.idle":"2023-08-14T11:59:29.458816Z","shell.execute_reply.started":"2023-08-14T11:59:29.458565Z","shell.execute_reply":"2023-08-14T11:59:29.458590Z"},"trusted":true},"execution_count":null,"outputs":[]}]}